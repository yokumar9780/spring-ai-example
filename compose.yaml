services:
  ollama-container:
    image: ollama/ollama:latest
    container_name: ollama
    networks:
      - local-ai-network
    ports:
      - "11434:11434"
    restart: always
    volumes:
      - ollama:/root/.ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [ gpu ]

  ollama-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: ollama-open-webui
    networks:
      - local-ai-network
    ports:
      - "4512:8080"
    restart: always
    volumes:
      - open-webui:/app/backend/data
    extra_hosts:
      - "host.docker.internal:host-gateway"
    environment:
      - USE_OLLAMA_DOCKER=true
      - OLLAMA_BASE_URL=http://ollama:11434   # Connects to Ollama API container
      - ALLOW_API_MANAGEMENT=true
      - ADMIN_USER=yogesh.kumar.3@volvo.com

# We create a specific volume for the Docker container to store the files related to this container
volumes:
  ollama:
  open-webui:

# We create a specific network for these Docker containers to communicate over called local-ai-network
networks:
  local-ai-network:
    driver: bridge
